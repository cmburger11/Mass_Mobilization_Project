{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial imports and declarations\n",
    "Will be expanded later to include a bevy of imports for various processing, data exploration, and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "#models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#post-modelling metrics\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"protests.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Structures \n",
    "Includes function declarations, lists, dictionaries, etc. that are used later in the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_drops = [\n",
    "    '1_accomodation', '1_arrests', '1_beatings', '1_crowd dispersal', '1_ignore', '1_killings', '1_shootings',\n",
    "    '2_accomodation', '2_arrests', '2_beatings', '2_crowd dispersal', '2_ignore', '2_killings', '2_shootings', \n",
    "    '3_accomodation', '3_arrests', '3_beatings', '3_crowd dispersal', '3_ignore', '3_killings', '3_shootings', \n",
    "    '4_accomodation', '4_arrests', '4_beatings', '4_crowd dispersal', '4_killings', '4_shootings', \n",
    "    '5_.', '5_accomodation', '5_arrests', '5_beatings', '5_crowd dispersal', '5_killings', '5_shootings', \n",
    "    '6_accomodation', '6_arrests', '6_beatings', '6_crowd dispersal', '6_killings', \n",
    "    '7_.', '7_accomodation', '7_arrests', '7_beatings', '7_killings'\n",
    "]\n",
    "\n",
    "demand_drops = [\n",
    "    'demand1_labor wage dispute', 'demand1_land farm issue', 'demand1_police brutality', 'demand1_political behavior, process', 'demand1_price increases, tax policy', 'demand1_removal of politician', 'demand1_social restrictions', \n",
    "    'demand2_labor wage dispute', 'demand2_land farm issue', 'demand2_police brutality', 'demand2_political behavior, process', 'demand2_price increases, tax policy', 'demand2_removal of politician', 'demand2_social restrictions', \n",
    "    'demand3_labor wage dispute', 'demand3_land farm issue', 'demand3_police brutality', 'demand3_political behavior, process', 'demand3_price increases, tax policy', 'demand3_removal of politician', 'demand3_social restrictions', \n",
    "    'demand4_.', 'demand4_labor wage dispute', 'demand4_land farm issue', 'demand4_police brutality', 'demand4_political behavior, process', 'demand4_price increases, tax policy', 'demand4_removal of politician'\n",
    "]\n",
    "\n",
    "time_drops = ['startday', 'startmonth', 'startyear', 'endday', 'endmonth', 'endyear']\n",
    "\n",
    "other_drops = [\n",
    "    'id', #Not useful to prediction.\n",
    "    'ccode', #Not useful to prediction.\n",
    "    'protest', #All values are 1.  Is this dataset the subset of another?\n",
    "    'protestnumber', # of protests per country might be useful but not in the context of incremental numbers that it's being given\n",
    "    'location', #Not extremely useable given how it's already being broken by region.\n",
    "    'participants_category', #Too many null values to be of great value.\n",
    "]\n",
    "\n",
    "demands = ['protesterdemand1', 'protesterdemand2', 'protesterdemand3', 'protesterdemand4']\n",
    "\n",
    "response = [\"stateresponse1\", \"stateresponse2\", \"stateresponse3\", \"stateresponse4\", \"stateresponse5\", \"stateresponse6\", \"stateresponse7\"]\n",
    "\n",
    "targets = ['y_accomodation', 'y_arrests', 'y_beatings', 'y_crowd dispersal', 'y_ignore', 'y_killings', 'y_shootings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_texts(x):\n",
    "    x = x.lower()\n",
    "    \n",
    "    if x == \"dozens\":\n",
    "        return 50\n",
    "    elif x == \"hundreds\":\n",
    "        return 500\n",
    "    elif x == \"thousands\":\n",
    "        return 5000\n",
    "    elif x == \"tens of thousands\":\n",
    "        return 50000\n",
    "    elif \"hundreds of thousands\" in x:\n",
    "        return 250000\n",
    "    elif \"millions\" in x:\n",
    "        return 2000000\n",
    "    elif \"million\" in x:\n",
    "        return 1000000\n",
    "    \n",
    "    \n",
    "    elif \"about \" in x:\n",
    "        return x[6:]\n",
    "    elif \"more than \" in x:\n",
    "        return x[10:]\n",
    "    \n",
    "    \n",
    "    elif \"several\" in x:\n",
    "        if \"dozen\" in x:\n",
    "            return 50\n",
    "        elif \"hundred\" in x:\n",
    "            return 500\n",
    "        elif \"thousand\" in x:\n",
    "            return 5000\n",
    "    \n",
    "    \n",
    "    elif \"hundreds\" in x:\n",
    "        return 500\n",
    "    elif \"thousands\" in x:\n",
    "        return 5000\n",
    "    \n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def strip_chars(x):\n",
    "    banned_chars = \"+s><,\"\n",
    "    x = \"\".join([c for c in x if c not in banned_chars])\n",
    "    \n",
    "    try:\n",
    "        x = int(x)\n",
    "    finally:\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "def avg_hyphen(x):\n",
    "    accepted_chars = \"1234567890-\"\n",
    "    ind = 0\n",
    "\n",
    "    x = \"\".join([c for c in x if c in accepted_chars])\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if x[i] == \"-\":\n",
    "            ind = i\n",
    "    \n",
    "    lower = x[:ind]\n",
    "    upper = x[ind+1:]\n",
    "    \n",
    "    if (lower == \"\") or (upper==\"\"):\n",
    "        return np.nan\n",
    "    \n",
    "    return (int(lower) + int(upper)) /2\n",
    "    \n",
    "    \n",
    "    \n",
    "def map_participants(x):\n",
    "    while type(x) == str:\n",
    "        x = parse_texts(x)\n",
    "        if type(x) == str:\n",
    "            x = strip_chars(x)\n",
    "        if type(x) == str:\n",
    "            x = avg_hyphen(x)\n",
    "        if type(x) == str:\n",
    "            x = np.nan\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Cleaning\n",
    "Contains blocks of code for known cleaning problems derived from any previous data exploration.\n",
    "\n",
    "To-do:\n",
    "1. Dummify and verticalize protestor demands.\n",
    "2. Rectify the participants_category, participants columns\n",
    "3. Get \"protest length\" as a feature\n",
    "4. drop id, ccode, protestnumber(?), sources(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General/Miscellaneous Cleaning\n",
    "\n",
    "df.dropna(subset=[\"notes\"], inplace=True) #If there are no notes, then we will not be able to predict the outcome very well.\n",
    "df.dropna(subset=[\"participants\"], inplace=True) #Participants had very few NaN values\n",
    "df.dropna(subset=[\"sources\"], inplace=True) #Sources had very few NaN values\n",
    "\n",
    "\n",
    "#Miscellaneous useless feature cleaning.  See the list declaration [other_drops] in DATA STRUCTURES for additional information.\n",
    "df.drop(columns=other_drops, inplace=True)\n",
    "\n",
    "\n",
    "#For the 500 or so values containing NaN in protestor identity:\n",
    "df.fillna(value={\"protesteridentity\":\"unspecified\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arcosion/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#For fixing the time values such that a length of time (in days) for the protest is established as a feature, and other time features are dropped.\n",
    "#Critically, the year the protest initially occured is retained in another column.\n",
    "\n",
    "month_days = {1:0, 2:31, 3:59, 4:90, 5:120, 6:151, 7:181, 8:212, 9:243, 10:273, 11:304, 12:334}\n",
    "df[\"protest_length\"] = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    yearday_start = month_days[df[\"startmonth\"].iloc[i]] + df[\"startday\"].iloc[i]\n",
    "    yearday_end = month_days[df[\"endmonth\"].iloc[i]] + df[\"endday\"].iloc[i]\n",
    "    \n",
    "    difference = (yearday_end - yearday_start) + (365 * (df[\"endyear\"].iloc[i] - df[\"startyear\"].iloc[i]))\n",
    "    \n",
    "    if difference != 0:\n",
    "        df[\"protest_length\"].iloc[i] = difference\n",
    "    else:\n",
    "        df[\"protest_length\"].iloc[i] = 1 #accounts for same-day protests\n",
    "\n",
    "\n",
    "#Now that the length is obtained, the additional time columns can be dropped.\n",
    "df.drop(columns=time_drops, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For fixing the Participants feature such that we have a numerical value.\n",
    "#For more information, see the function map_participants() in DATA STRUCTURES.\n",
    "df[\"participants\"] = df[\"participants\"].map(map_participants)\n",
    "\n",
    "df.dropna(subset=[\"participants\"], inplace=True) #150 null values remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For translating the vertical state response & the protester demands values laterally.\n",
    "\n",
    "\n",
    "df = pd.get_dummies(data=df, prefix=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"], columns=response)\n",
    "df = pd.get_dummies(data=df, prefix=[\"demand1\", \"demand2\", \"demand3\", \"demand4\"], columns=demands)\n",
    "\n",
    "\n",
    "#Combining the disparate dummies into unified response columns.  \n",
    "#Unfortunately there was a certain amount of manual labor involved in this due to how finicky pandas is.\n",
    "df[\"demand_labor_wage_dispute\"] = df['demand1_labor wage dispute'] + df['demand2_labor wage dispute'] + df['demand3_labor wage dispute'] + df['demand4_labor wage dispute']\n",
    "df[\"demand_land_farm_issue\"] = df['demand1_land farm issue'] + df['demand2_land farm issue'] + df['demand3_land farm issue'] + df['demand4_land farm issue']\n",
    "df[\"demand_police_brutality\"] = df['demand1_police brutality'] + df['demand2_police brutality'] + df['demand3_police brutality'] + df['demand4_police brutality']\n",
    "df[\"demand_political_behavior_or_process\"] = df['demand1_political behavior, process'] + df['demand2_political behavior, process'] + df['demand3_political behavior, process'] + df['demand4_political behavior, process']\n",
    "df[\"demand_price_hike_or_tax_policy\"] = df['demand1_price increases, tax policy'] + df['demand2_price increases, tax policy'] + df['demand3_price increases, tax policy'] + df['demand4_price increases, tax policy']\n",
    "df[\"demand_removal_of_politician\"] = df['demand1_removal of politician'] + df['demand2_removal of politician'] + df['demand3_removal of politician'] + df['demand4_removal of politician']\n",
    "df[\"demand_social_restrictions\"] = df['demand1_social restrictions'] + df['demand2_social restrictions'] + df['demand3_social restrictions']\n",
    "\n",
    "df[\"y_accomodation\"] = df['1_accomodation'] + df['2_accomodation'] + df['3_accomodation'] + df['4_accomodation'] + df['5_accomodation'] + df['6_accomodation'] + df['7_accomodation']\n",
    "df[\"y_arrests\"] = df['1_arrests'] + df['2_arrests'] + df['3_arrests'] + df['4_arrests'] + df['5_arrests'] + df['6_arrests'] + df['7_arrests']\n",
    "df[\"y_beatings\"] = df['1_beatings'] + df['2_beatings'] + df['3_beatings'] + df['4_beatings'] + df['5_beatings'] + df['6_beatings'] + df['7_beatings']\n",
    "df[\"y_crowd_dispersal\"] = df['1_crowd dispersal'] + df['2_crowd dispersal'] + df['3_crowd dispersal'] + df['4_crowd dispersal'] + df['5_crowd dispersal'] + df['6_crowd dispersal']\n",
    "df[\"y_ignore\"] = df['1_ignore'] + df['2_ignore'] + df['3_ignore']\n",
    "df[\"y_killings\"] = df['1_killings'] + df['2_killings'] + df['3_killings'] + df['4_killings'] + df['5_killings'] + df['6_killings'] + df['7_killings']\n",
    "df[\"y_shootings\"] = df['1_shootings'] + df['2_shootings'] + df['3_shootings'] + df['4_shootings'] + df['5_shootings']\n",
    "\n",
    "\n",
    "\n",
    "#Getting rid of the disparate dummies now that we have unified responses.\n",
    "df.drop(columns=response_drops, inplace=True)\n",
    "df.drop(columns=demand_drops, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, dropping Oceania due to the limited number of entries for that region.\n",
    "df = df[df[\"region\"] != \"Oceania\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data exploration & analysis\n",
    "Find problems to address here and then address them in the data cleaning section.  Or, create graphs or other data exploration methods here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14264 entries, 0 to 16312\n",
      "Data columns (total 23 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   country                               14264 non-null  object \n",
      " 1   year                                  14264 non-null  int64  \n",
      " 2   region                                14264 non-null  object \n",
      " 3   protesterviolence                     14264 non-null  float64\n",
      " 4   participants                          14264 non-null  float64\n",
      " 5   protesteridentity                     14264 non-null  object \n",
      " 6   sources                               14264 non-null  object \n",
      " 7   notes                                 14264 non-null  object \n",
      " 8   protest_length                        14264 non-null  float64\n",
      " 9   demand_labor_wage_dispute             14264 non-null  uint8  \n",
      " 10  demand_land_farm_issue                14264 non-null  uint8  \n",
      " 11  demand_police_brutality               14264 non-null  uint8  \n",
      " 12  demand_political_behavior_or_process  14264 non-null  uint8  \n",
      " 13  demand_price_hike_or_tax_policy       14264 non-null  uint8  \n",
      " 14  demand_removal_of_politician          14264 non-null  uint8  \n",
      " 15  demand_social_restrictions            14264 non-null  uint8  \n",
      " 16  y_accomodation                        14264 non-null  uint8  \n",
      " 17  y_arrests                             14264 non-null  uint8  \n",
      " 18  y_beatings                            14264 non-null  uint8  \n",
      " 19  y_crowd_dispersal                     14264 non-null  uint8  \n",
      " 20  y_ignore                              14264 non-null  uint8  \n",
      " 21  y_killings                            14264 non-null  uint8  \n",
      " 22  y_shootings                           14264 non-null  uint8  \n",
      "dtypes: float64(3), int64(1), object(5), uint8(14)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Modelling\n",
    "\n",
    "Objective:  Optimize the model selection through gridsearch. Then append dataframe results.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_responses = ['y_accomodation', 'y_arrests', 'y_beatings', 'y_crowd_dispersal', 'y_ignore', 'y_killings', 'y_shootings']\n",
    "\n",
    "region_list = dict(df[\"region\"].value_counts()).keys()\n",
    "\n",
    "country_list = dict(df[\"country\"].value_counts()).keys()\n",
    "\n",
    "all_features = ['year', 'protesterviolence', 'participants', 'protest_length',\n",
    "       'demand_labor_wage_dispute', 'demand_land_farm_issue',\n",
    "       'demand_police_brutality', 'demand_political_behavior_or_process',\n",
    "       'demand_price_hike_or_tax_policy', 'demand_removal_of_politician',\n",
    "       'demand_social_restrictions', 'notes']\n",
    "\n",
    "num_features = ['year', 'protesterviolence', 'participants', 'protest_length',\n",
    "       'demand_labor_wage_dispute', 'demand_land_farm_issue',\n",
    "       'demand_police_brutality', 'demand_political_behavior_or_process',\n",
    "       'demand_price_hike_or_tax_policy', 'demand_removal_of_politician',\n",
    "       'demand_social_restrictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg__protest_by_the_response(df, response):    \n",
    "    X = df[all_features]\n",
    "    y = df[response]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    \n",
    "    get_numeric_data = FunctionTransformer(lambda df: df[num_features], validate=False)\n",
    "    get_text_data = FunctionTransformer(lambda df: df['notes'], validate=False)\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer(stop_words='english'))\n",
    "            ]))\n",
    "         ])),\n",
    "    ('log', LogisticRegression(max_iter=5000))\n",
    "    ])\n",
    "    \n",
    "    params = {\n",
    "        'log__penalty' : ['l2', 'l1'],\n",
    "#        'log__C' : [0.001, 0.01, 0,1, 1, 5],\n",
    "#        'features__text_features__cvec__max_df': [0.90, 0.95],\n",
    "#        'features__text_features__cvec__max_features': [None, 1000, 3000, 5000],\n",
    "        'log__solver' : ['liblinear']\n",
    "    }\n",
    "    \n",
    "\n",
    "    gs = GridSearchCV(pipe, param_grid=params, cv=5, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    baseline = max([df[response].mean(), 1-df[response].mean()])\n",
    "    \n",
    "    reg_response_dict = {\n",
    "        \"model_type\": \"logreg\",\n",
    "        \"region\": df[\"region\"].iloc[0],\n",
    "        \"training_score\": gs.score(X_train, y_train),\n",
    "        \"testing_score\": gs.score(X_test, y_test),\n",
    "        \"baseline\": baseline,\n",
    "        \"baseline_response\": (response[2:] if df[response].mean() > 0.5 else f\"no {response[2:]}\"),\n",
    "        \"model_success\": (\"yes\" if gs.score(X_test, y_test) > (0.05+baseline) else \"no\"),\n",
    "        \"best_params\": gs.best_params_\n",
    "    }\n",
    "    \n",
    "    return reg_response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf__protest_by_the_response(df, response):\n",
    "    X = df[all_features]\n",
    "    y = df[response]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    \n",
    "    get_numeric_data = FunctionTransformer(lambda df: df[num_features], validate=False)\n",
    "    get_text_data = FunctionTransformer(lambda df: df['notes'], validate=False)\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer(stop_words='english'))\n",
    "            ]))\n",
    "         ])),\n",
    "    ('rf', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    params = {\n",
    "#        'rf__ccp_alpha' : [0.001, 0.01, 0.1, 1, 5],\n",
    "        'rf__n_estimators' : [100, 300],\n",
    "        'rf__max_depth' : [None, 5, 10],\n",
    "        'rf__min_samples_split' : [2, 4],\n",
    "        'rf__min_samples_leaf' : [1, 3]\n",
    "    }\n",
    "    \n",
    "    gs = GridSearchCV(pipe, param_grid=params, cv=5, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    baseline = max([df[response].mean(), 1-df[response].mean()])\n",
    "            \n",
    "    reg_response_dict = {\n",
    "        \"model_type\": \"rf\",\n",
    "        \"region\": df[\"region\"].iloc[0],\n",
    "        \"training_score\": gs.score(X_train, y_train),\n",
    "        \"testing_score\": gs.score(X_test, y_test),\n",
    "        \"baseline\": baseline,\n",
    "        \"baseline_response\": (response[2:] if df[response].mean() > 0.5 else f\"no {response[2:]}\"),\n",
    "        \"model_success\": (\"yes\" if gs.score(X_test, y_test) > (0.05+baseline) else \"no\"),\n",
    "        \"best_params\": gs.best_params_\n",
    "    }\n",
    "    \n",
    "    return reg_response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc__protest_by_the_response(df, response):\n",
    "    X = df[all_features]\n",
    "    y = df[response]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    \n",
    "    get_numeric_data = FunctionTransformer(lambda df: df[num_features], validate=False)\n",
    "    get_text_data = FunctionTransformer(lambda df: df['notes'], validate=False)\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer(stop_words='english'))\n",
    "            ]))\n",
    "         ])),\n",
    "    ('svc', SVC())\n",
    "    ])\n",
    "    \n",
    "    params = {\n",
    "        'svc__C' : [5],\n",
    "        'svc__degree' :[2],\n",
    "    }\n",
    "    \n",
    "    gs = GridSearchCV(pipe, param_grid=params, cv=5, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    baseline = max([df[response].mean(), 1-df[response].mean()])\n",
    "            \n",
    "    reg_response_dict = {\n",
    "        \"model_type\": \"svc\",\n",
    "        \"region\": df[\"region\"].iloc[0],\n",
    "        \"training_score\": gs.score(X_train, y_train),\n",
    "        \"testing_score\": gs.score(X_test, y_test),\n",
    "        \"baseline\": baseline,\n",
    "        \"baseline_response\": (response[2:] if df[response].mean() > 0.5 else f\"no {response[2:]}\"),\n",
    "        \"model_success\": (\"yes\" if gs.score(X_test, y_test) > (0.05+baseline) else \"no\"),\n",
    "        \"best_params\": gs.best_params_\n",
    "    }\n",
    "    \n",
    "    return reg_response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb__protest_by_the_response(df, response):\n",
    "    X = df[all_features]\n",
    "    y = df[response]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    \n",
    "    get_numeric_data = FunctionTransformer(lambda df: df[num_features], validate=False)\n",
    "    get_text_data = FunctionTransformer(lambda df: df['notes'], validate=False)\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer(stop_words='english'))\n",
    "            ]))\n",
    "         ])),\n",
    "    ('xg', XGBClassifier())\n",
    "    ])\n",
    "    \n",
    "    params = {\n",
    "#        'xg__gamma' : [0.001, 0.01, 0.1, 1, 5],\n",
    "        'xg__max_depth' :[None, 2, 3],\n",
    "#        'xg__learning_rate' : [0.001, 0.01, 0.1, 1, 5]\n",
    "    }\n",
    "    \n",
    "    gs = GridSearchCV(pipe, param_grid=params, cv=5, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    baseline = max([df[response].mean(), 1-df[response].mean()])\n",
    "            \n",
    "    reg_response_dict = {\n",
    "        \"model_type\": \"xgb\",\n",
    "        \"region\": df[\"region\"].iloc[0],\n",
    "        \"training_score\": gs.score(X_train, y_train),\n",
    "        \"testing_score\": gs.score(X_test, y_test),\n",
    "        \"baseline\": baseline,\n",
    "        \"baseline_response\": (response[2:] if df[response].mean() > 0.5 else f\"no {response[2:]}\"),\n",
    "        \"model_success\": (\"yes\" if gs.score(X_test, y_test) > (0.05+baseline) else \"no\"),\n",
    "        \"best_params\": gs.best_params_\n",
    "    }\n",
    "    \n",
    "    return reg_response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Remaining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = pd.DataFrame(columns=[\"model_type\", \"region\", \"baseline_response\", \"training_score\", \"testing_score\", \"baseline\", \"model_success\", \"best_params\"])\n",
    "\n",
    "\n",
    "\n",
    "def logreg__responses_by_location(loc_df):\n",
    "    return [logreg__protest_by_the_response(loc_df, response) for response in possible_responses]\n",
    "\n",
    "def rf__responses_by_location(loc_df):\n",
    "    return [rf__protest_by_the_response(loc_df, response) for response in possible_responses]\n",
    "\n",
    "def svc__responses_by_location(loc_df):\n",
    "    return [svc__protest_by_the_response(loc_df, response) for response in possible_responses]\n",
    "\n",
    "def xgb__responses_by_location(loc_df):\n",
    "    return [xgb__protest_by_the_response(loc_df, response) for response in possible_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_by_region(df, region):\n",
    "    return df[df[\"region\"]==region]\n",
    "\n",
    "def df_by_country(df, country):\n",
    "    return df[df[\"country\"]==country]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Running the models\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_list:\n",
    "    for dct in logreg__responses_by_location(df_by_region(df, region)):\n",
    "        grid_results = grid_results.append(dct, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_list:\n",
    "    for dct in rf__responses_by_location(df_by_region(df, region)):\n",
    "        grid_results = grid_results.append(dct, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_list:\n",
    "    for dct in svc__responses_by_location(df_by_region(df, region)):\n",
    "        grid_results = grid_results.append(dct, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_list:\n",
    "    for dct in xgb__responses_by_location(df_by_region(df, region)):\n",
    "        grid_results = grid_results.append(dct, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>region</th>\n",
       "      <th>baseline_response</th>\n",
       "      <th>training_score</th>\n",
       "      <th>testing_score</th>\n",
       "      <th>baseline</th>\n",
       "      <th>model_success</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_type, region, baseline_response, training_score, testing_score, baseline, model_success, best_params]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_results.to_csv(\"./data/four_models.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Engineering & Assessment\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results[\"success_rate\"] = grid_results[\"testing_score\"] - grid_results[\"baseline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = grid_results[grid_results[\"model_type\"] == \"logreg\"][\"success_rate\"].mean()\n",
    "rf = grid_results[grid_results[\"model_type\"] == \"rf\"][\"success_rate\"].mean()\n",
    "svc = grid_results[grid_results[\"model_type\"] == \"svc\"][\"success_rate\"].mean()\n",
    "xgb = grid_results[grid_results[\"model_type\"] == \"xgb\"][\"success_rate\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Logistic Regression:\\t{lg}\")\n",
    "print(f\"Random Forest:\\t\\t{rf}\")\n",
    "print(f\"SVC:\\t\\t\\t{svc}\")\n",
    "print(f\"XGB Classifier:\\t\\t{xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Additional Modelling\n",
    "---\n",
    "---\n",
    "Consolidating the possible protest outcomes into broader, more predictable categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_drops = ['y_arrests', 'y_crowd_dispersal', 'y_beatings', 'y_killings', 'y_shootings']\n",
    "\n",
    "df[\"y_adverse_reaction\"] = df[\"y_arrests\"] + df[\"y_crowd_dispersal\"]\n",
    "df[\"y_adverse_reaction\"] = df[\"y_adverse_reaction\"].map(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "df[\"y_state_violence\"] = df[\"y_beatings\"] + df[\"y_killings\"] + df[\"y_shootings\"]\n",
    "df[\"y_state_violence\"] = df[\"y_state_violence\"].map(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "df.drop(columns=new_drops, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_responses = ['y_accomodation', 'y_ignore', 'y_adverse_reaction', 'y_state_violence']\n",
    "possible_demands = ['demand_labor_wage_dispute', 'demand_land_farm_issue',\n",
    "       'demand_police_brutality', 'demand_political_behavior_or_process',\n",
    "       'demand_price_hike_or_tax_policy', 'demand_removal_of_politician',\n",
    "       'demand_social_restrictions']\n",
    "\n",
    "for r in possible_responses:\n",
    "    df[r] = df[r].map(lambda x: 1 if x>0 else 0)\n",
    "    \n",
    "for d in possible_demands:\n",
    "    df[d] = df[d].map(lambda x: 1 if x>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'year', 'region', 'protesterviolence', 'participants',\n",
       "       'protesteridentity', 'sources', 'notes', 'protest_length',\n",
       "       'demand_labor_wage_dispute', 'demand_land_farm_issue',\n",
       "       'demand_police_brutality', 'demand_political_behavior_or_process',\n",
       "       'demand_price_hike_or_tax_policy', 'demand_removal_of_politician',\n",
       "       'demand_social_restrictions', 'y_accomodation', 'y_ignore',\n",
       "       'y_adverse_reaction', 'y_state_violence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb__protest_by_the_response(df, response):\n",
    "    xgb_features = all_features + [\"country\", \"protesteridentity\", \"sources\"]\n",
    "    text_features = [\"country\", \"protesteridentity\", \"sources\", \"notes\"]\n",
    "    \n",
    "    X = df[xgb_features]\n",
    "    y = df[response]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    \n",
    "    get_numeric_data = FunctionTransformer(lambda df: df[num_features], validate=False)\n",
    "    get_text_data = FunctionTransformer(lambda df: df[\"notes\"], validate=False)\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('ss', StandardScaler())\n",
    "            ])),\n",
    "             ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('cvec', CountVectorizer(stop_words='english', max_df=0.8, max_features=2000))\n",
    "            ]))\n",
    "         ])),\n",
    "    ('xg', XGBClassifier())\n",
    "    ])\n",
    "    \n",
    "    params = {\n",
    "#        'xg__gamma' : [0.001, 0.01, 0.1, 1, 5],\n",
    "        'xg__max_depth' :[2],\n",
    "        'xg__learning_rate' : [.1],        \n",
    "    }\n",
    "    \n",
    "    gs = GridSearchCV(pipe, param_grid=params, cv=5, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    baseline = max([df[response].mean(), 1-df[response].mean()])\n",
    "            \n",
    "    reg_response_dict = {\n",
    "        \"model_type\": \"xgb\",\n",
    "        \"region\": df[\"region\"].iloc[0],\n",
    "        \"training_score\": gs.score(X_train, y_train),\n",
    "        \"testing_score\": gs.score(X_test, y_test),\n",
    "        \"baseline\": baseline,\n",
    "        \"baseline_response\": (response[2:] if df[response].mean() > 0.5 else f\"no {response[2:]}\"),\n",
    "        \"model_success\": (\"yes\" if gs.score(X_test, y_test) > (0.05+baseline) else \"no\"),\n",
    "        \"best_params\": gs.best_params_\n",
    "    }\n",
    "    \n",
    "    return reg_response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb__responses_by_location(loc_df):\n",
    "    return [xgb__protest_by_the_response(loc_df, response) for response in possible_responses]\n",
    "\n",
    "def df_by_region(df, region):\n",
    "    return df[df[\"region\"]==region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = pd.DataFrame(columns=[\"model_type\", \"region\", \"baseline_response\", \"training_score\", \"testing_score\", \"baseline\", \"model_success\"])\n",
    "\n",
    "\n",
    "for region in region_list:\n",
    "    for dct in xgb__responses_by_location(df_by_region(df, region)):\n",
    "        xgb_results = xgb_results.append(dct, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>region</th>\n",
       "      <th>baseline_response</th>\n",
       "      <th>training_score</th>\n",
       "      <th>testing_score</th>\n",
       "      <th>baseline</th>\n",
       "      <th>model_success</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Europe</td>\n",
       "      <td>no accomodation</td>\n",
       "      <td>0.945373</td>\n",
       "      <td>0.934002</td>\n",
       "      <td>0.923302</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Europe</td>\n",
       "      <td>ignore</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.852130</td>\n",
       "      <td>0.676280</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Europe</td>\n",
       "      <td>no adverse_reaction</td>\n",
       "      <td>0.899387</td>\n",
       "      <td>0.883876</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Europe</td>\n",
       "      <td>no state_violence</td>\n",
       "      <td>0.970736</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.960084</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Africa</td>\n",
       "      <td>no accomodation</td>\n",
       "      <td>0.915344</td>\n",
       "      <td>0.892999</td>\n",
       "      <td>0.873719</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Africa</td>\n",
       "      <td>no ignore</td>\n",
       "      <td>0.835979</td>\n",
       "      <td>0.816380</td>\n",
       "      <td>0.585785</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Africa</td>\n",
       "      <td>no adverse_reaction</td>\n",
       "      <td>0.867284</td>\n",
       "      <td>0.837517</td>\n",
       "      <td>0.516033</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Africa</td>\n",
       "      <td>no state_violence</td>\n",
       "      <td>0.891975</td>\n",
       "      <td>0.869221</td>\n",
       "      <td>0.793719</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Asia</td>\n",
       "      <td>no accomodation</td>\n",
       "      <td>0.893883</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.876303</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Asia</td>\n",
       "      <td>ignore</td>\n",
       "      <td>0.803058</td>\n",
       "      <td>0.768056</td>\n",
       "      <td>0.505907</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Asia</td>\n",
       "      <td>no adverse_reaction</td>\n",
       "      <td>0.816960</td>\n",
       "      <td>0.793056</td>\n",
       "      <td>0.628562</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Asia</td>\n",
       "      <td>no state_violence</td>\n",
       "      <td>0.906395</td>\n",
       "      <td>0.852778</td>\n",
       "      <td>0.846421</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgb</td>\n",
       "      <td>South America</td>\n",
       "      <td>no accomodation</td>\n",
       "      <td>0.917727</td>\n",
       "      <td>0.882952</td>\n",
       "      <td>0.887405</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xgb</td>\n",
       "      <td>South America</td>\n",
       "      <td>ignore</td>\n",
       "      <td>0.801527</td>\n",
       "      <td>0.758270</td>\n",
       "      <td>0.567430</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xgb</td>\n",
       "      <td>South America</td>\n",
       "      <td>no adverse_reaction</td>\n",
       "      <td>0.840543</td>\n",
       "      <td>0.862595</td>\n",
       "      <td>0.646947</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>xgb</td>\n",
       "      <td>South America</td>\n",
       "      <td>no state_violence</td>\n",
       "      <td>0.942324</td>\n",
       "      <td>0.923664</td>\n",
       "      <td>0.922392</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>xgb</td>\n",
       "      <td>MENA</td>\n",
       "      <td>no accomodation</td>\n",
       "      <td>0.943280</td>\n",
       "      <td>0.918819</td>\n",
       "      <td>0.917745</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>xgb</td>\n",
       "      <td>MENA</td>\n",
       "      <td>no ignore</td>\n",
       "      <td>0.843403</td>\n",
       "      <td>0.785978</td>\n",
       "      <td>0.560074</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>xgb</td>\n",
       "      <td>MENA</td>\n",
       "      <td>no adverse_reaction</td>\n",
       "      <td>0.855734</td>\n",
       "      <td>0.789668</td>\n",
       "      <td>0.617375</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xgb</td>\n",
       "      <td>MENA</td>\n",
       "      <td>no state_violence</td>\n",
       "      <td>0.874229</td>\n",
       "      <td>0.782288</td>\n",
       "      <td>0.752311</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xgb</td>\n",
       "      <td>North America</td>\n",
       "      <td>no accomodation</td>\n",
       "      <td>0.937669</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.910751</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>xgb</td>\n",
       "      <td>North America</td>\n",
       "      <td>ignore</td>\n",
       "      <td>0.883469</td>\n",
       "      <td>0.782258</td>\n",
       "      <td>0.576065</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xgb</td>\n",
       "      <td>North America</td>\n",
       "      <td>no adverse_reaction</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.830645</td>\n",
       "      <td>0.667343</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>xgb</td>\n",
       "      <td>North America</td>\n",
       "      <td>no state_violence</td>\n",
       "      <td>0.945799</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.890467</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Central America</td>\n",
       "      <td>no accomodation</td>\n",
       "      <td>0.956386</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Central America</td>\n",
       "      <td>ignore</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.599068</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Central America</td>\n",
       "      <td>no adverse_reaction</td>\n",
       "      <td>0.931464</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>xgb</td>\n",
       "      <td>Central America</td>\n",
       "      <td>no state_violence</td>\n",
       "      <td>0.956386</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.850816</td>\n",
       "      <td>no</td>\n",
       "      <td>{'xg__learning_rate': 0.1, 'xg__max_depth': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_type           region    baseline_response  training_score  \\\n",
       "0         xgb           Europe      no accomodation        0.945373   \n",
       "1         xgb           Europe               ignore        0.862319   \n",
       "2         xgb           Europe  no adverse_reaction        0.899387   \n",
       "3         xgb           Europe    no state_violence        0.970736   \n",
       "4         xgb           Africa      no accomodation        0.915344   \n",
       "5         xgb           Africa            no ignore        0.835979   \n",
       "6         xgb           Africa  no adverse_reaction        0.867284   \n",
       "7         xgb           Africa    no state_violence        0.891975   \n",
       "8         xgb             Asia      no accomodation        0.893883   \n",
       "9         xgb             Asia               ignore        0.803058   \n",
       "10        xgb             Asia  no adverse_reaction        0.816960   \n",
       "11        xgb             Asia    no state_violence        0.906395   \n",
       "12        xgb    South America      no accomodation        0.917727   \n",
       "13        xgb    South America               ignore        0.801527   \n",
       "14        xgb    South America  no adverse_reaction        0.840543   \n",
       "15        xgb    South America    no state_violence        0.942324   \n",
       "16        xgb             MENA      no accomodation        0.943280   \n",
       "17        xgb             MENA            no ignore        0.843403   \n",
       "18        xgb             MENA  no adverse_reaction        0.855734   \n",
       "19        xgb             MENA    no state_violence        0.874229   \n",
       "20        xgb    North America      no accomodation        0.937669   \n",
       "21        xgb    North America               ignore        0.883469   \n",
       "22        xgb    North America  no adverse_reaction        0.934959   \n",
       "23        xgb    North America    no state_violence        0.945799   \n",
       "24        xgb  Central America      no accomodation        0.956386   \n",
       "25        xgb  Central America               ignore        0.897196   \n",
       "26        xgb  Central America  no adverse_reaction        0.931464   \n",
       "27        xgb  Central America    no state_violence        0.956386   \n",
       "\n",
       "    testing_score  baseline model_success  \\\n",
       "0        0.934002  0.923302            no   \n",
       "1        0.852130  0.676280           yes   \n",
       "2        0.883876  0.727273           yes   \n",
       "3        0.964912  0.960084            no   \n",
       "4        0.892999  0.873719            no   \n",
       "5        0.816380  0.585785           yes   \n",
       "6        0.837517  0.516033           yes   \n",
       "7        0.869221  0.793719           yes   \n",
       "8        0.883333  0.876303            no   \n",
       "9        0.768056  0.505907           yes   \n",
       "10       0.793056  0.628562           yes   \n",
       "11       0.852778  0.846421            no   \n",
       "12       0.882952  0.887405            no   \n",
       "13       0.758270  0.567430           yes   \n",
       "14       0.862595  0.646947           yes   \n",
       "15       0.923664  0.922392            no   \n",
       "16       0.918819  0.917745            no   \n",
       "17       0.785978  0.560074           yes   \n",
       "18       0.789668  0.617375           yes   \n",
       "19       0.782288  0.752311            no   \n",
       "20       0.935484  0.910751            no   \n",
       "21       0.782258  0.576065           yes   \n",
       "22       0.830645  0.667343           yes   \n",
       "23       0.903226  0.890467            no   \n",
       "24       0.925926  0.916084            no   \n",
       "25       0.740741  0.599068           yes   \n",
       "26       0.851852  0.666667           yes   \n",
       "27       0.814815  0.850816            no   \n",
       "\n",
       "                                       best_params  \n",
       "0   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "1   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "2   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "3   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "4   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "5   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "6   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "7   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "8   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "9   {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "10  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "11  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "12  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "13  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "14  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "15  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "16  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "17  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "18  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "19  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "20  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "21  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "22  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "23  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "24  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "25  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "26  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  \n",
       "27  {'xg__learning_rate': 0.1, 'xg__max_depth': 2}  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Success Rate above baseline:\t\t\t\t0.10625412697572266\n",
      "XGB Success Rate above baseline (for baselines under 85%):\t0.17200293795994087\n"
     ]
    }
   ],
   "source": [
    "xgb_results[\"success_rate\"] = xgb_results[\"testing_score\"] - xgb_results[\"baseline\"]\n",
    "\n",
    "xgb_total_success = xgb_results[\"success_rate\"].mean()\n",
    "xgb_low_success = xgb_results[xgb_results[\"baseline\"]<0.85][\"success_rate\"].mean()\n",
    "\n",
    "print(f\"XGB Success Rate above baseline:\\t\\t\\t\\t{xgb_total_success}\")\n",
    "print(f\"XGB Success Rate above baseline (for baselines under 85%):\\t{xgb_low_success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14264 entries, 0 to 16312\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   country                               14264 non-null  object \n",
      " 1   year                                  14264 non-null  int64  \n",
      " 2   region                                14264 non-null  object \n",
      " 3   protesterviolence                     14264 non-null  float64\n",
      " 4   participants                          14264 non-null  float64\n",
      " 5   protesteridentity                     14264 non-null  object \n",
      " 6   sources                               14264 non-null  object \n",
      " 7   notes                                 14264 non-null  object \n",
      " 8   protest_length                        14264 non-null  float64\n",
      " 9   demand_labor_wage_dispute             14264 non-null  int64  \n",
      " 10  demand_land_farm_issue                14264 non-null  int64  \n",
      " 11  demand_police_brutality               14264 non-null  int64  \n",
      " 12  demand_political_behavior_or_process  14264 non-null  int64  \n",
      " 13  demand_price_hike_or_tax_policy       14264 non-null  int64  \n",
      " 14  demand_removal_of_politician          14264 non-null  int64  \n",
      " 15  demand_social_restrictions            14264 non-null  int64  \n",
      " 16  y_accomodation                        14264 non-null  int64  \n",
      " 17  y_ignore                              14264 non-null  int64  \n",
      " 18  y_adverse_reaction                    14264 non-null  int64  \n",
      " 19  y_state_violence                      14264 non-null  int64  \n",
      "dtypes: float64(3), int64(12), object(5)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'protesterviolence',\n",
       " 'participants',\n",
       " 'protest_length',\n",
       " 'demand_labor_wage_dispute',\n",
       " 'demand_land_farm_issue',\n",
       " 'demand_police_brutality',\n",
       " 'demand_political_behavior_or_process',\n",
       " 'demand_price_hike_or_tax_policy',\n",
       " 'demand_removal_of_politician',\n",
       " 'demand_social_restrictions',\n",
       " 'notes']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10139, 20) (14264, 20)\n"
     ]
    }
   ],
   "source": [
    "df2000 = df[df[\"year\"]>1999]\n",
    "\n",
    "print(df2000.shape, df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
